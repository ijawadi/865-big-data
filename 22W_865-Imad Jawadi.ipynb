{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMA 2021S 865, Individual Assignment 1\n",
    "\n",
    "Version 2: Updated Janurary 13, 2021.\n",
    "\n",
    "- [Imad, Jawadi]\n",
    "- [20197485]\n",
    "- [Section#: 1]\n",
    "- [Good to Great]\n",
    "- [10-19-2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1 - ELI5\n",
    "\n",
    "_“If you can't explain it simply, you don't understand it well enough.” – Albert Einstein_\n",
    "\n",
    "Explaining technical concepts to a non-technical audience is an underappreciated skill; one which the MMA program aims to give its students; and one that will truly set you apart in the job market. The only way to gain a skill is by practice, so here we go.\n",
    "\n",
    "Answer each question below as though you were talking to a 5 year old (equivalently: a grandma, or a completely non-technical manager, or an Ivey grad). Use your own words. Use analogies where possible. Examples are better than theory. Keep it short, but be complete. Use simple, plain English. Do not use business buzzwords like _actualize, empower, fungible, leverage, or synergize_. Do not use technical buzzwords that most people don’t know like _model, agile, bandwidth, IoT, blockchain, AR, VR, actionable insights_. Inform the audience without going into too much technical detail, and without embarrassing yourself. Your goal is to truly help them understand, not to give what you feel is a “technically precise” answer and move on (but they still don’t understand!). Don’t be that guy!\n",
    "\n",
    "Please keep each answer to 1000 characters or less.\n",
    "\n",
    "Finally, feel free to use [Markdown syntax](https://www.markdownguide.org/basic-syntax/) to format your answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: What is “Big Data” and how is it different than “regular data”?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here.\n",
    "\n",
    "### What is Big Data?\n",
    "<p>Let me explain Big Data with anexample.  If you go on Facebook and sayyou click on stories, and watch some friends' stories, skip some, addemojis to some stories, and comment on a few.<br>  Every single click, text, skip, repeat, forward, backward of story clips are captured by Facebook.<br> Facebook also captures when you log on, how long you stay on its website, the phrases you search, and if you click on a friend, group, ad or news item that you click at.</p>\n",
    "\n",
    "<p>All of those individual data points come togetherto paint a picture about you, your hobbies, friends, what you like ordislike, your political point of view, what you browsed, and ultimately who you are!.<br>\n",
    "\n",
    "Captured from billions of users every click, Facebook creates what is called \"big data.\"<br>\n",
    "This data then is sliced and diced by Facebook to identify \"who you are, what your interests are and what makesyou tick\".</p>\n",
    "\n",
    "<p>Facebook then identifies content that you may like and/or influence your decision-making process.<br>\n",
    "These choices could be as simple as purchasing a nail cutter or as pronounced as sending you customized advertisements so feed you specific baised/fake news to influnce who you vote for!</p>\n",
    "\n",
    "### Why is it called \"Big Data?\" \n",
    "<p>This is called Big Data because a company like Facebook collects huge amounts of data for every single Facebook user.<br>\n",
    "\n",
    "The data is in **different formats** plain text (your comments), numbers (# of likes, # of video clips watched, during of each clip, how much of a clip was watched and how many clips were skipped, time spend each session), streaming data (data that was gathered out of your browsing behaviour from websites), voice and video data. </p>\n",
    "\n",
    "Each piece of data, or core information, will require **specific treatment**. <br> In addition, eachtype of data will require **specific storage needs**, for example, the storage of ane-mail will be much less than that of a video.<br> The true value of big data comes by building models that provide an organization, say Facebook a broader view of their customer and business by tapping into different and previously unuseddata sources.</p>\n",
    "\n",
    "<p>These models can accurately predict details about each user thatcould never be imagined before, like personality type, political affiliation, health, behaviour, etc.<br>  And these models in turn lead to more **educated and informed decisions** with the use ofanalytics.</p>\n",
    "\n",
    "### What is \"regular data?\" \n",
    "<p>The monthly credit card or bank statement,annual mortgage statement, semi-annual property tax statement, bi-weekly incomestatements from employer, or your child's report card, are examples of regulardata.<br>  Regular data is referred to datathat is structured, and it can be stored in a spreadsheet (or for large volumesin a database) and can be stored and processed by an organization.<br> This information is useful and has beenaround for decades, but has limited use in predicting future actions.<br> Regular data does not include audio, video and emojis.</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: What is Hadoop? Hint: What problems in previous data storage and processing was Hadoop designed to solve? How did Hadoop accomplish that?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here.\n",
    "\n",
    "### internet created huge volumes of data and different data formats \n",
    "<p> As the World Wide Web grew in the 2000s, the concept of \"searching the internet\" was born.<br>  Users wanted to type a phrase and see links to relevant websites.<br>  This in turn created companies dedicated to search.<br>  The internet search companies were indexing websites to help locate relevant information amid the text-based content. </p>  \n",
    "\n",
    "The internet growth was unmatched in human history and search engines became critical, giving birth to start-ups like Yahoo, AskJeeve, AltaVista and later on Google.<br>\n",
    "The search engine companies had a few issues to deal with:<br>\n",
    "1. Users wanted **accurate, relevant and \"fast\" response** <br>\n",
    "2. The content on the internet was **growing at unbelievable rate** <br>\n",
    "3. Storing data and search computation required an *insatiable amount of hardware*.<br>\n",
    "\n",
    "These issues made researchers think about innovative ways to **split data and computation across different computers** in order to return web search results faster.<br>  In other words, break the search task into multiple, smaller tasks that run simultaneously.<br>  This is *how Hadoop was born* and **Hadoop could be summarized as distributed computing and processing**.<br>\n",
    "\n",
    "Hadoop (named was created by Cutting, an engineer at Yahoo and Hadoop was his son’s toy elephant).<br> In 2008, Yahoo released Hadoop as an open-source project and today Hadoop’s framework and ecosystem of technologies are managed and maintained by the non-profit Apache Software Foundation.</p> \n",
    "\n",
    "Hadoop is designed to handle the three attributes of big data: volume, variety, velocity<br> \n",
    "1. **Volume**: Hadoop is a distributed architecture that scales cost-effectively. Hadoop was designed to scale out (as supposed to scale up).  Scale-out is much more cost-effective. As you need more storage or computing capacity, all you need to do is add more computers (or nodes) to the cluster.<br> \n",
    "2. **Variety**: Hadoop allows you to store data in any format, structured or unstructured. This means that you will not need to alter your data to fit any single schema before putting it into Hadoop.<br> \n",
    "3. **Velocity**:  With Hadoop, you can load raw data into the system and then later define how you want to view it. Because of the flexibility of the Hadoop system, you are able to avoid many network and processing bottlenecks associated with loading raw data. Since data is always changing, the flexibility of the system makes it much easier to integrate future changes.<br>\n",
    "\n",
    "Other attributes that make Hadoop important:<br>\n",
    "4. **Fault tolerance**: A major issue with distributing computing tasks to run across different computers is dealing with the possibility of node(s) failing.<br>  Hadoop is designed to protect against hardware failure.<br> If a node goes down, Hadoop jobs are automatically redirected to other nodes to make sure the distributed computing does not fail.<br>\n",
    "6. **Assume node failure is common**: Hadoop assumed that nodes will fail on a regular basis and therefore it embedded a process that stores multiple copies of data automatically.<br>\n",
    "7. **Low cost**: Since Hadoop is an open-source framework, it's free.  Furthermore, Hadoop uses commodity hardware to store large quantities of data.<br>  \n",
    "8. **Reduce administrative headache**:  Hadoop was designed to hide system-level details from programmers.<br> \n",
    "9. **Move processing to data**: In traditional non-big-data environments, there are two kinds of machines, optimized to store data and optimized to compute.<br>  *Big data was a different animal and it introduced unforeseen challenges particularly data transfer caused huge bottlenecks*.<br>  Hadoop fixed this problem by bringing the code (computation) to data.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: How does Big Data and the cloud help Machine Learning? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here.\n",
    "\n",
    "\n",
    "**Big Data** and **cloud computing** go hand-in-hand, with many public cloud services performing big data analytics.<br> \n",
    "\n",
    "*What are big data and cloud computing and why do these two terms frequently appear together?* <br> \n",
    "In a nutshell, **big data** refers to the large sets of data collected, while **cloud computing** refers to the mechanism that remotely takes this data in and performs any operations specified on that data.<br> \n",
    "\n",
    "Cloud computing providers often utilize a *software as a service* model to allow customers to easily process data.<br> Typically, a console that can take in specialized commands and parameters is available, as almost everything can be done from the site’s user interface.<br> Some cloud providers bundle relevant products as packages that include database management systems, cloud-based virtual machines/containers, identity management systems, machine learning capabilities, etc.<br>\n",
    "\n",
    "Applications of machine learning are also fueled by big data. *Without big data, there would be far fewer machine learning applications or cloud-based services.* <br> Remember, machine learning is powered by bid data and the cloud enables machine learning's data collection, workload, and computation.<br> \n",
    "\n",
    "### Advantages of cloud on machine learning:\n",
    "It should be noted that open-source and proprietary machine-learning systems have been around for many years.<br> But the cost of these systems, in terms of hardware (on-prem dedicated servers) and software (like SAS), were out of reach for most enterprises.<br> Moreover, even if a business could afford it, it typically did not have the machine-learning talent required to design the prediction models or deal with the data science required.<br>  *Let me illustrate by sharing my personal experience:*<br>\n",
    "About five years ago, I worked on a data mining project.<br>  We spoke with an unnamed company and to get software with data mining capabilities.<br>  It cost us **USD 500,000** for two named users.<br>  We were also told that we were renting the software and not acquiring licenses!<br>  We were paying rent for data mining capabilities and the software vendor would send us annual authorization \"key\"!<br>  The project cost us USD 1,200,000.<br>  I happened to have the same dataset and run it on my laptop using Python in the summer of 2021.<br>  The cost was **n$0.00 for software** (since Python is free) and approximately $10 in utility to run it on my laptop!<br>\n",
    "\n",
    "#### Pay-per-use: <br>\n",
    "Cloud Computing also allows us to use state-of-the-art infrastructure and *only pay for the time and power that we use!* <br> Cloud’s pay-per-use model is incredibly attractive for artificial intelligence or machine learning workloads.<br> The cloud makes it easy for enterprises to experiment with machine learning capabilities and scale up as projects go into production and demand increases.<br> The cloud also makes *intelligent and user-friendly capabilities accessible without requiring advanced skills in artificial intelligence or data science.* <br> Data Robots, AWS, Microsoft, Google, IBM are a few examples that offer many machine learning options that don’t require deep knowledge of AI, machine learning theory, or a team of data scientists.<br> \n",
    "\n",
    "#### Scale with Cloud:  <br>\n",
    "Cloud also addresses workload scaling issues since training real-world models typically require large compute clusters.<br>\n",
    "\n",
    "#### Integration with other systems:  <br>\n",
    "The cloud providers also provide **SDKs** (software developer kits) and **APIs** that embed machine-learning functionality directly into applications.<br> This is important as some applications of machine learning predictions are related to operations and in particular \"transaction-focused,\" as the ability to determine in real-time if a credit card transaction is likely to be fraudulent and provide real-time access to deal with the issue to decline fraudulent transactions.<br>\n",
    "\n",
    "#### Advantages of big data on machine learning: <br>\n",
    "**Troves of data:** A major source of big data is the internet as it provides a level of concrete information about consumer habits, likes and dislikes, activities, and personal preferences.<br>  Social media accounts and online profiles, social activity, product reviews, tagged interests, “liked” and shared content, loyalty, rewards apps and programs, and customer relationship management systems all add potentially insightful data to the big data pool.<br>\n",
    "\n",
    "#### Bid data enables machine's learning capabilities: <br>\n",
    "Machine learning’s greatest asset is its *learning ability*. <br>Its capacity to recognize trends in data is only useful if it can adapt to changes and fluctuations in those trends. <br>Big data is used to collect, digest and process data. <br>\n",
    "\n",
    "**Constantly collect data from different sources:** <br> Machine learning’s ability to work with data analytics is the primary reason why \"machine learning and big data\" are now seemingly inseparable.<br>  Machine learning and other disciplines of AI, like deep learning are pulling from every data input and using those inputs to generate new rules for future business analytics. <br>  To illustrate with an *example*, online shopping is particularly reliant on data, so they’re using machine learning to provide real-time insights on customer feedback. <br>Using machine learning, online retailer businesses can form their finances, strategies, and marketing around the flow of new information.<br>\n",
    "\n",
    "*Machine learning and big data complement each other to automate ML workflows:* <br>\n",
    "Machine Learning provides efficient and automated tools for data gathering, analysis, and assimilation. <br>In collaboration with cloud computing, machine learning agility ingest data into processing and integrates large amounts of data regardless of source and format. <br>The data is fed into the ML engine, making the model smarter, with accurate predictions. <br>Over time, and with more relevant data, less and less human intervention would be required for a machine learning model to run properly. <br>\n",
    "\n",
    "#### Big data helps avoid model drift:<br>\n",
    "For the ML models to avoid drift, stay relevant and provide accurate and useful predictions, their algorithms **require massive amounts of data**. <br> Natural language processing, for example, as we noticed in our MMA 865 Group Project, would not be possible without millions of reviews text, helpfulness votes by other users and finally long and in-depth reviews.<br>  The ML algorithms digest all this information, it then breaks the data down into a format that algorithms can understand and process.<br>  We also noticed that the *age of data* could be relevant and important. <br> An AI practitioner compared data to **milk** and stated some algorithms need fresh data, like fresh milk and old data can negatively impact predictions. <br> He stated that in those algorithms it's critical to remove aged/old data on a regular basis, to avoid model drift.<br>  So in the context of big data, machine learning algorithms are used to keep up or improvise by themselves with the ever-growing and ever-changing streams of data and continuously deliver predictions or insights.<br>\n",
    "\n",
    "**In summary,** the fusion of machine learning and big data is a never-ending loop. <br> The algorithms created for certain purposes are monitored and perfected over time as the information is coming into the system and out of the system.<br>\n",
    "\n",
    "For machine learning to run effectively, there is always a need for more big data.  High-tech enterprises use a  **three-goal approach** to ensure a **constant flow of big data into their models**.<br>  Here is how it works:<br>\n",
    "1. **Engagmenetn goal**: <br>Ensure users are engaged in your portal (Amazon, Facebook, YouTube, Twitter, Netflix, Instagram etc).  The longer a user stays, the more data is generated.<br>\n",
    "2. **Growth Goal**: <br> With growth in mind, users are encouraged to invite/connect with other users.  <br>This further increases engagement and also adds more users to the system. <br> The growth goal elevates individual engagement to community engagement which in turn generates more data by more users.  Another term for growth goal is known as \"the network effect\" <br>\n",
    "3. **Advertisement Goal**: <br>The advertisement goal harnesses big data, using cloud storage and compute power with machine learning model to monetize data. <br> The monetization could be different based on business models. <br> It could recommend product purchases, movies or videos to watch, which some call magic. <br> \n",
    "\n",
    "\n",
    "My wife told me a few months back:<br>\n",
    "\"You know what, Amazon suggested a book that I didn't even know existed.  I bought and read it and it was amazing.\" **This is magic.** <br>    \n",
    "This phenomenon is beautifully articled by Arthur Clarke as: \"Any sufficiently advances technology is indistinguishable from magic.\" <br>\n",
    "\n",
    "Machine learning, cloud and big data have enabled high-tech companies to create magic and their evaluations prove their magic work!<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: What is NoSQL?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here.\n",
    "\n",
    "#### What is a NoSQL database?\n",
    "When we use the term “NoSQL database,” we typically use it to refer to any \"non-relational database\". <br>Some say the term “NoSQL” stands for “non SQL” while others say it stands for “not only SQL.”<br> The bottom line is NoSQL databases are databases that store data in a format other than relational tables.<br>\n",
    "\n",
    "**Background:**<br>\n",
    "For the last five decades of SQL databases, the focus was to create a \"schema\" optimized to store data efficiently.<br> The schema required careful planning to estimate table growth, size and business need to create a proper database that avoids data duplications and optimizes storage cost. <br>\n",
    "\n",
    "In the late 2000s, as online users exploded and the concept of social media took hold, developers noticed that although the storage costs have been consistently decreasing, the amount of data that applications needed to store and query was increasing at a much higher rate. <br> Furthermore, data format was different (structured, semi-structured, and polymorphic) and data size also varied significantly. <br> Information experts realized that defining database schemas in advance was impossible. <br> This gives birth to a number of different database types which allows flexibility where huge amounts of unstructured data could be easily stored and retrieved. <br> These databases are called NoSQL.<br>\n",
    "\n",
    "*Note that NoSQL has different business usages and they do not replace SQL databases.* A key feature of SQL database is strong **ACID**, where ACID refers to the four key properties of a transaction: Atomicity, Consistency, Isolation, and Durability.<br>  ACID guarantee is critical in the record management and bank transaction systems. <br> These guarantees are also cost-intensive. <br> An example of ACID guarantee is that in a shared bank account if one person withdraws money, the balance is guaranteed to be updated immediately to ensure balance accuracy. <br><br> Many features of social media applications do not require ACID guarantee, for example, if one writes a comment on Facebook, Facebook does not guarantee an immediate/accurate count of 'likes' on the comment.<br>\n",
    "\n",
    "*Other factors that helped NoSQL databases* were the rise of Agile development, and the need to rapidly adapt to changing requirements. <br> Developers needed the ability to iterate quickly and make changes throughout the software development cycle — from code all the way down to the database. NoSQL databases gave them this flexibility.<br>\n",
    "\n",
    "As cloud computing was embraced by developers, and applications were hosted on the clouds, there was a need to distribute data across multiple servers and regions to make applications resilient and to scale out instead of scale-up, and intelligently store data based on geo-place usage. <br>\n",
    "\n",
    "*This is why NoSQL databases come into existence*.<br>\n",
    "Contrary to SQL databases, a NoSQL database does not organize data as rows and columns.<br>  It uses a model that is optimized for the \"specific business need/requirements\" of the type of data being stored.<br>\n",
    "There are four main NoSQL database types: Key-Value, Graph, Document and Columnar.  Each one of these databases is optimized for a specific purpose.<br>\n",
    "\n",
    "##### 1. Key-value:<br>\n",
    "The key-value data store is essentially a very large lookup table.<br> It stores data as a collection of key-value pairs in which a key serves as a unique identifier. Both keys and values can be anything, ranging from simple objects to complex compound objects. <br>The main feature of this datastore is scalability and they're used mostly to store customer profiles, preferences and configurations. <br> They're also used in blockchain implementation. Almost all multimedia storage or large objects (video, images, audio, etc.) uses key-value data storage.<br>\n",
    "\n",
    "##### 2. Graph databases:<br>\n",
    "A graph database enables businesses to combine multiple data sources (customer information, purchase history, production information, clickstream data, online reviews, like/dislike etc) into a single dynamic model to get a 360 view of an object (that could be a person, product, company etc) and run similar algorithms to recommend new products and services to the right customers at the right time.<br>\n",
    "\n",
    "Graph databases store, query, and manage networks and they make analyzing networks easier and faster to analyze \n",
    "My data is a graph.<br>\n",
    "\n",
    "Graph databases are used for fraud detection, 360 views of customers, recommendation engines. network/operations mapping, social networks and supply chain mapping.<br>\n",
    "\n",
    "\n",
    "##### 3. Document database:<br>\n",
    "A document database is a type of nonrelational database that is designed to store and query data as JSON, XML docs, catalogues, serialized PDFs, Excel docs, Profile data, and serialized objects. <br>Document databases make it easier for developers to store and query data in a database by using the same document-model format they use in their application code.<br>\n",
    "\n",
    "A document-based database is recommended to use when you do not need to store data in tables with uniform-sized fields for each record. <br>Instead, you have a need to store each record as a document that has certain characteristics.<br>\n",
    "\n",
    "A document database is a great choice for content management applications such as blogs and video platforms. <br> With a document database, each entity that the application tracks can be stored as a single document. <br> The document database is more intuitive for a developer to update an application as the requirements evolve. <br> In addition, if the data model needs to change, only the affected documents need to be updated. No schema update is required and no database downtime is necessary to make the changes. <br>\n",
    "\n",
    "##### 4. Columnar database:<br>\n",
    "Columnar databases are used in data warehouses where businesses send massive amounts of data from multiple sources for BI analysis. <br>Columnar database stores data by columns rather than by rows, which makes it suitable for analytical query processing, and thus for data warehouses. <br> Columnar databases are widely adopted by business intelligence (BI) practitianures.<br>  Column-oriented databases have faster query performance because the column design keeps data closer together, which reduces seek time. Columnar database are fully SQL compliant. <br>\n",
    "\n",
    "\n",
    "\n",
    "**Summary of  ACID guarantee:** <br>\n",
    "**1. Atomicity:** <br> all changes to data are performed as if they are a single operation or none of them are performed. <br> For example, in an application that transfers funds from one account to another, the atomicity property ensures that, if a debit is made successfully from one account, the corresponding credit is made to the other account.<br>\n",
    "**2. Consistency:** <br> Data is in a consistent state when a transaction starts and when it ends. <br>For example, in an application that transfers funds from one account to another, the consistency property ensures that the total value of funds in both the accounts is the same at the start and end of each transaction. <br>\n",
    "**3. Isolation:** <br>  The intermediate state of a transaction is invisible to other transactions. <br>As a result, transactions that run concurrently appear to be serialized.  For example, in an application that transfers funds from one account to another, the isolation property ensures that another transaction sees the transferred funds in one account or the other, but not in both, nor in neither.<br>\n",
    "**4. Durability:** <br> After a transaction successfully completes, changes to data persist and are not undone, even in the event of a system failure. For example, in an application that transfers funds from one account to another, the durability property ensures that the changes made to each account will not be reversed.<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Name three ways topic modeling could help a bank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here.\n",
    "\n",
    "**There are many ways that topic modelling could help a bank.  Here are a few:** <br>\n",
    "*1.  Top Modelling and Risk Management:Topic modelling can be an effective tool in ﬁnancial decision-making and risk management.* <br>\n",
    "There is an enormous amount of text related to banks that can be harnessed by available sources like reports, regulatory ﬁlings, news, social media, chat rooms, and discussion boards. <br>\n",
    "Topic modelling can take advantage of text data for ﬁnancial and risk management.  <br>The topic modelling can be modelled to identify risk-related topics and potentially complement it with quantitative data and fine-tune decision-making processes.  <br>\n",
    "\n",
    "*2.  Topic Modelling and equity/ foreign exchange:* <br>\n",
    "Top modelling can also be used in banking to predict equity or currency price movement. For example, financial institutions can use news articles to predict how the stock market will react or analyze localized news and its impact on foreign exchange markets. <br> A great example of this could've been analyzing pro-government Chinese news articles that started criticizing fintech enterprises in February 2021 and in a few weeks shaved off over two billion dollars from their market capitalization! <br>\n",
    "\n",
    "*3. Topic Modelling to monitor social media, finance message boards and their impact on companies stock market:*<br>\n",
    "Topic modelling can also be utilized by analyzing social media text from Twitter, Linked In, Google and Yahoo Finance message boards.<br>  These platforms are used by members to compliment or criticize products and also vendors use these platforms to fetch customer feedback, request product features, announce upgrades, bug fixes etc.  Topic modelling can be used to predict how the market would respond to social media posts. <br>\n",
    "\n",
    "* 4. Topic modelling and macroeconomic outlooks:* <br>\n",
    "Another potential application of topic modelling and text mining could be to measure ﬁnancial conditions, such as credit rating prediction, bank distress prediction and systemic risk measurement.  <br>\n",
    "\n",
    "*5. Apply topic modelling using phone and chat data to improve customer satisfaction:* <br>\n",
    "Using topic modelling, banks can use their chat and phone data to discover customer issues, pain points and complaints and improve customer satisfaction. <br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: What is Apache Spark, exactly, and what are its pros and cons?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here.\n",
    "\n",
    "In machine learning there are two major challenges that developers are faced with: **scaling model size** and **scaling data size**.  <br>\n",
    "Scaling model size refer to models becoming comlex or grwoing so large the it affects affects the  workflow. <br> The scaling challenge includes model training, prediction, or evaluation steps. <br> The model may take too long to complete these steps or it would halt or crash the system. <br> The compute parallelization is a recommended to address this challnege. <br>\n",
    "\n",
    "The data scaling challenge relates to datasets volumes that cosume so much RAM the the system is paralzied and failes to load the dataset. <br> This challenge requies dataset to be distributed into different nodes and this why products like **Spark** is created to solve model or data size challenges. <br>\n",
    "\n",
    "\n",
    "*Spark is a platform that allows for large-scale, cluster-based, in-memory data processing.* <br> It enables fast, large-scale data engineering and analytics for iterative and performance-sensitive applications. <br> It offers development APIs for Scala, Java, Python, and R. <br>\n",
    "\n",
    "In addition, and *probably what made Spark extremely popular* is the fact that it is extended to support SQL-like operations and since SQL is widely used, SQL developers have embraced Spark which in turn made Spark's popularity skyrocket.<br>\n",
    "\n",
    "Apache Spark Core includes integration with Microsoft Azure, auto-scaling, multi-language notebooks, built-in machine learning frameworks and integrated security. <br>It is also designed to process its computation in memory of the various nodes and it works with a distributed file system. <br>\n",
    "\n",
    "Spark itself is an ever-growing ecosystem. <br>The Spark Core API includes integration with popular languages like R, SQL, Python, Scala, and Java. <br> Sitting on top of that are higher-level frameworks where we can work with Spark SQL and DataFrames, **Streaming data**, **Mllib** for Machine Learning and **GraphX** for Graph Computation.<br>\n",
    "\n",
    "#### Benefits of Apache Spark: <br>\n",
    "There are many positive attributes about Spark, here is a summary that I found most useful:<br>\n",
    "1. **Speed:**<br> Spark is using \"all in-memory\" computation, no disk access, which makes it extremely fast. <br>\n",
    "2. **Increased access to big data:** <br> Spark's general engine is built and optimized for large-scale data processing. <br> Spark's engine combined with in-memory computation makes it 100+ times faster than similar tools in big data space.<br>\n",
    "3. **Spark platform is multilingual and open-source.** <br>  It supports many popular languages and licensing cost is negligible.\n",
    "4. **Ease of Use:** <br> Although Spark is a sophisticated platform, a lot of development efforts are put into this platform so to ensure built-in support: <br>\n",
    "    **a) SQL-like operations**<br>\n",
    "    **b) Machine learning algorithms**<br>\n",
    "    **c) Streaming data**<br>\n",
    "    **d) Graph processing**<br>\n",
    "    **e) Popular programming languages like Scala, Java, Python and R.** <br>\n",
    "    **f) Spark abstracts/automates admin tasks so programmers can mainly focus on data science activities** <br>\n",
    "***These built-in supports are key in making Spark the “best in show” tool for Hadoop and Big Data*** <br>\n",
    " \n",
    "Although Spark is great, it has its **shortcomings**.  Here are a few: <br>\n",
    " **1. Spark lacks a robust and mature automatic optimization process** <br>\n",
    " **2. No File Management System:** <br>\n",
    " Spark has no file management system of its own and it generally depends on some other file management systems. So, it needs to merge with one, either HDFS or other cloud-based data platforms. <br>\n",
    " **3. Spark does not support real-time processing.** <br>\n",
    " Using Spark streaming, the live data which arrives is automatically divided into batches. <br> Those batches are of the pre-defined interval, then each batch of data is handled as Spark Resilient Distributed Datasets, \"RDDs\". <br> Afterwards, these RDDs processed using the operations like map, reduce, join etc. As data is divided into batches, their results are also returned in batches. <br> So in Spark streaming, micro-batch processing takes place, which is \"near real-time\" processing of live data. <br>\n",
    "  **4. The Mllib for Machine Learning: **<br>\n",
    " MLLibis not as mature as Python packages.<br> As of now, it contains few algorithms. <br>\n",
    "  **5. Spark is open-source and it lacks formal support if needed.** <br>  This is important as most organizations will not use software that lacks dedicated Service Level Agreement and support. \n",
    "\n",
    "*That's why companies like Databricks are created that enhance Spark capabilities and furthermore provide support for Apache Spark that is hosted on a cloud system like Microsoft Azure.* <br>\n",
    "\n",
    "#### Why Databricks is a perfect match for Spark? <br>\n",
    "Spark, in a nutshell, is around distributed computing, and really what's driving it is the volume of data. <br> The ML tasks that developers use Spark for including data cleansing, or extract, transform, and load (ETL); fast data serving pipelines; scalable complex processing; and distributed machine learning. <br>\n",
    "\n",
    "*One can think of Azure Databricks as a set of three components:* <br>\n",
    "1. Databricks tools, services, and optimizations that surround the core open-source Apache Spark distribution, <br>\n",
    "2. Apache Spark itself provides the distributed computation needed for these intensive workloads, and <br>\n",
    "3. this sits on top of some sort of file system. <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2: Sentiment Analysis via the ML-based approach\n",
    "\n",
    "Download the “Product Sentiment” dataset from the course portal: sentiment_train.csv and sentiment_test.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a. Loading and Prep\n",
    "\n",
    "Load, clean, and preprocess the data as you find necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 0.0 import libraries and write functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing Required Python Libraries and Packages\n",
      "Requirement already satisfied: numpy in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (1.19.5)\n",
      "Requirement already satisfied: pandas in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (1.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pandas) (1.19.5)\n",
      "Requirement already satisfied: six>=1.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: sklearn in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from sklearn) (0.23.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.5.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.19.5)\n",
      "Requirement already satisfied: spacy in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (3.1.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.8.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: jinja2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.0.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (4.59.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (2.4.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (8.0.10)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.7.4)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (1.19.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (0.6.0)\n",
      "Requirement already satisfied: setuptools in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy) (57.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (3.7.4.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy) (1.1.1)\n",
      "Requirement already satisfied: nltk in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: regex in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: click in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from beautifulsoup4) (2.2.1)\n",
      "Requirement already satisfied: notebook-as-pdf in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (0.5.0)\n",
      "Requirement already satisfied: PyPDF2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from notebook-as-pdf) (1.26.0)\n",
      "Requirement already satisfied: nbconvert in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from notebook-as-pdf) (6.0.7)\n",
      "Requirement already satisfied: pyppeteer in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from notebook-as-pdf) (0.2.6)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (0.8.4)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (0.5.3)\n",
      "Requirement already satisfied: jinja2>=2.4 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (2.11.3)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (0.1.2)\n",
      "Requirement already satisfied: defusedxml in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (0.7.1)\n",
      "Requirement already satisfied: bleach in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (3.3.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (5.0.5)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (2.8.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (1.4.3)\n",
      "Requirement already satisfied: testpath in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (0.4.4)\n",
      "Requirement already satisfied: jupyter-core in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (4.7.1)\n",
      "Requirement already satisfied: nbformat>=4.4 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbconvert->notebook-as-pdf) (5.1.3)\n",
      "Requirement already satisfied: websockets<10.0,>=9.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer->notebook-as-pdf) (9.1)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer->notebook-as-pdf) (8.2.2)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer->notebook-as-pdf) (1.26.4)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer->notebook-as-pdf) (3.10.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer->notebook-as-pdf) (4.59.0)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer->notebook-as-pdf) (1.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=1.4->pyppeteer->notebook-as-pdf) (3.4.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jinja2>=2.4->nbconvert->notebook-as-pdf) (1.1.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jupyter-client>=6.1.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook-as-pdf) (6.1.12)\n",
      "Requirement already satisfied: async-generator in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook-as-pdf) (1.10)\n",
      "Requirement already satisfied: nest-asyncio in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook-as-pdf) (1.5.1)\n",
      "Requirement already satisfied: ipython-genutils in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->notebook-as-pdf) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from nbformat>=4.4->nbconvert->notebook-as-pdf) (3.2.0)\n",
      "Requirement already satisfied: packaging in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook-as-pdf) (20.9)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook-as-pdf) (1.15.0)\n",
      "Requirement already satisfied: webencodings in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from bleach->nbconvert->notebook-as-pdf) (0.5.1)\n",
      "Requirement already satisfied: setuptools in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->notebook-as-pdf) (57.5.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->notebook-as-pdf) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.4->nbconvert->notebook-as-pdf) (20.3.0)\n",
      "Requirement already satisfied: tornado>=4.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert->notebook-as-pdf) (6.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert->notebook-as-pdf) (2.8.1)\n",
      "Requirement already satisfied: pyzmq>=13 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert->notebook-as-pdf) (20.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from packaging->bleach->nbconvert->notebook-as-pdf) (2.4.7)\n",
      "Requirement already satisfied: pyppeteer in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (0.2.6)\n",
      "Requirement already satisfied: urllib3<2.0.0,>=1.25.8 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer) (1.26.4)\n",
      "Requirement already satisfied: appdirs<2.0.0,>=1.4.3 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer) (1.4.4)\n",
      "Requirement already satisfied: pyee<9.0.0,>=8.1.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer) (8.2.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer) (4.59.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer) (3.10.0)\n",
      "Requirement already satisfied: websockets<10.0,>=9.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pyppeteer) (9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from importlib-metadata>=1.4->pyppeteer) (3.4.1)\n",
      "Collecting en-core-web-sm==3.1.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.1.0/en_core_web_sm-3.1.0-py3-none-any.whl (13.6 MB)\n",
      "Requirement already satisfied: spacy<3.2.0,>=3.1.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from en-core-web-sm==3.1.0) (3.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.7.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (20.9)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.19.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.8.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.0.5)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.6.0)\n",
      "Requirement already satisfied: jinja2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.11.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.25.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.8.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.59.0)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (57.5.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.9 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (8.0.10)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (3.7.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.26.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (7.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/ijawadi/opt/anaconda3/lib/python3.8/site-packages (from jinja2->spacy<3.2.0,>=3.1.0->en-core-web-sm==3.1.0) (1.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Importing Libraies and Packages\n",
      "Defining Functions to Pre-process Text Data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ijawadi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# TODO: import other libraries as necessary\n",
    "# In this section: I decided not to use piplines and isntead code so I can fully understand sentiment analysis.\n",
    "# A summary of this section includes \n",
    "# 1. I installed libraries\n",
    "# 2. Import libraries\n",
    "# 3. Import scikit-learn Modules for Data Modeling\n",
    "# 4. Import SpaCy Modules \n",
    "# 5. Define a few Functions to Pre-process Text Data\n",
    "# 6.  and Contraction Map, and expand it\n",
    "# 7. other fuctions include strip html tages\n",
    "# 8.  remove special characters\n",
    "# 9. lemmatize text, remove stopwprds, normalize document  and corpus\n",
    "# 10. and finally evaluate model \n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Install Required Python Libraries and Packages\n",
    "print('Installing Required Python Libraries and Packages')\n",
    "os.system('pip install numpy')\n",
    "os.system('pip install pandas')\n",
    "os.system('pip install sklearn')\n",
    "os.system('pip install spacy')\n",
    "os.system('pip install nltk')\n",
    "os.system('pip install beautifulsoup4')\n",
    "os.system('pip install -U notebook-as-pdf')\n",
    "os.system('pip install pyppeteer')\n",
    "os.system('python -m spacy download en_core_web_sm')\n",
    "\n",
    "\n",
    "# Import Libraies and Packages\n",
    "print('Importing Libraies and Packages')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import unicodedata\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "# Import scikit-learn Modules for Data Modeling\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "# SpaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nltk.download('stopwords')\n",
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "stopword_list.remove('no')\n",
    "stopword_list.remove('not')\n",
    "tokenizer = ToktokTokenizer()\n",
    "\n",
    "# Define Functions to Pre-process Text Data\n",
    "print('Defining Functions to Pre-process Text Data')\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "# Contraction Map\n",
    "CONTRACTION_MAP = {\n",
    "\"ain't\": \"is not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he'll've\": \"he will have\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"I'd\": \"I would\",\n",
    "\"I'd've\": \"I would have\",\n",
    "\"I'll\": \"I will\",\n",
    "\"I'll've\": \"I will have\",\n",
    "\"I'm\": \"I am\",\n",
    "\"I've\": \"I have\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'd've\": \"i would have\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'll've\": \"i will have\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it'll've\": \"it will have\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she'll've\": \"she will have\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they'll've\": \"they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what'll've\": \"what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who'll've\": \"who will have\",\n",
    "\"who's\": \"who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you'll've\": \"you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n",
    "\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char + expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text()\n",
    "    return stripped_text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r'[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    text = nlp(text)\n",
    "    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "    return text\n",
    "\n",
    "def remove_stopwords(text, is_lower_case=False):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    if is_lower_case:\n",
    "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    else:\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def normalize_doc(\n",
    "                doc, html_stripping=True, contraction_expansion=True,\n",
    "                accented_char_removal=True, text_lower_case=True,\n",
    "                text_lemmatization=True, special_char_removal=True,\n",
    "                stopword_removal=True):\n",
    "\n",
    "                doc = remove_accented_chars(doc)\n",
    "                doc = expand_contractions(doc)\n",
    "\n",
    "                # strip HTML\n",
    "                if html_stripping:\n",
    "                    doc = strip_html_tags(doc)\n",
    "                # remove accented characters\n",
    "                if accented_char_removal:\n",
    "                    doc = remove_accented_chars(doc)\n",
    "                # expand contractions\n",
    "                if contraction_expansion:\n",
    "                    doc = expand_contractions(doc)\n",
    "                # lowercase the text\n",
    "                if text_lower_case:\n",
    "                    doc = doc.lower()\n",
    "                # remove extra newlines\n",
    "                doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "                # insert spaces between special characters to isolate them\n",
    "                special_char_pattern = re.compile(r'([{.(-)!}])')\n",
    "                doc = special_char_pattern.sub(\" \\\\1 \", doc)\n",
    "                # lemmatize text\n",
    "                if text_lemmatization:\n",
    "                    doc = lemmatize_text(doc)\n",
    "                # remove special characters\n",
    "                if special_char_removal:\n",
    "                    doc = remove_special_characters(doc)\n",
    "                # remove extra whitespace\n",
    "                doc = re.sub(' +', ' ', doc)\n",
    "                # remove stopwords\n",
    "                if stopword_removal:\n",
    "                    doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n",
    "\n",
    "                return doc\n",
    "\n",
    "\n",
    "def normalize_corpus(corpus, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True,\n",
    "                     text_lemmatization=True, special_char_removal=True,\n",
    "                     stopword_removal=True):\n",
    "\n",
    "    normalized_corpus = []\n",
    "    # normalize each document in the corpus\n",
    "    for doc in corpus:\n",
    "\n",
    "        doc = normalize_doc(doc, html_stripping, contraction_expansion,\n",
    "                            accented_char_removal, text_lower_case,\n",
    "                            text_lemmatization, special_char_removal,\n",
    "                            stopword_removal)\n",
    "\n",
    "        normalized_corpus.append(doc)\n",
    "\n",
    "    return normalized_corpus\n",
    "\n",
    "\n",
    "def model_evaluation(model, Xtest, y_test, dataset='Test'):\n",
    "    y_preds = model.predict(Xtest)\n",
    "    y_preds_probs = model.predict_proba(Xtest)[:, 1]\n",
    "    print('Accuracy of {} Set is'.format(dataset), accuracy_score(y_test, y_preds))\n",
    "    print(\"\\n\")\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_preds).ravel()\n",
    "    print('True Negative = {}\\tFalse Positive = {}\\nFalse Negative = {}\\tTrue Positive = {}'\n",
    "          .format(tn, fp, fn, tp))\n",
    "    print(\"\\n\")\n",
    "    print(classification_report(y_test, y_preds))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.a. Loading and Prep\n",
    "\n",
    "Load, clean, and preprocess the data as you find necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Training Data and Normalizing it\n",
      "Reading Test Data and Normalizing it\n"
     ]
    }
   ],
   "source": [
    "# to ensure code could run on any environment, I moved datasets to my GitHub page\n",
    "print('Reading Training Data and Normalizing it')\n",
    "train_data_file = 'https://raw.githubusercontent.com/ijawadi/865-big-data/master/sentiment_train.csv'\n",
    "train_data = pd.read_csv(train_data_file)\n",
    "\n",
    "train_sentences = np.array(train_data['Sentence'])\n",
    "norm_train_sentences = normalize_corpus(train_sentences)\n",
    "\n",
    "print('Reading Test Data and Normalizing it')\n",
    "test_data_file = 'https://raw.githubusercontent.com/ijawadi/865-big-data/master/sentiment_test.csv'\n",
    "test_data = pd.read_csv(test_data_file)\n",
    "\n",
    "test_sentences = np.array(test_data['Sentence'])\n",
    "norm_test_sentences = normalize_corpus(test_sentences)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.b. Modeling\n",
    "\n",
    "Use your favorite ML algorithm to train a classification model.  Don’t forget everything that we’ve learned in our ML course: hyperparameter tuning, cross validation, handling imbalanced data, etc. Make reasonable decisions and try to create the best-performing classifier that you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "## Creating Features from Text using BOW and TF-IDF.   \n",
    "# These are techniques that help us convert text sentences into numeric vectors\n",
    "\n",
    "# Build BOW [BagOfWords] features on Train Sentences\n",
    "cv = CountVectorizer(min_df=1, ngram_range=(1, 3))\n",
    "X_train_CV = cv.fit_transform(norm_train_sentences)\n",
    "\n",
    "# build TF-IDF [Term Frequency - Inverse Term frequency] Features on Train Sentences\n",
    "tv = TfidfVectorizer(min_df=1, ngram_range=(1, 3))\n",
    "X_train_Tf = tv.fit_transform(norm_train_sentences)\n",
    "\n",
    "y_train = list(train_data['Polarity'])\n",
    "\n",
    "# Transform Test Sentences into Features\n",
    "X_test_CV = cv.transform(norm_test_sentences)\n",
    "X_test_Tf = tv.transform(norm_test_sentences)\n",
    "\n",
    "y_test = list(test_data['Polarity'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c. Assessing\n",
    "\n",
    "Use the testing data to measure the accuracy and F1-score of your model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train Features Shape: (2400, 22294)  Test Features Shape: (600, 22294)\n",
      "TF-IDF model:> Train Features Shape: (2400, 22294)  Test Features Shape: (600, 22294)\n",
      "Training and Evaluating Random Forests Classifier on CV Features\n",
      "Best parameters are: {'C': 10, 'solver': 'newton-cg'}\n",
      "Best F1 score is: 0.9081556974758535\n",
      "Accuracy of Test Set is 0.7883333333333333\n",
      "\n",
      "\n",
      "True Negative = 247\tFalse Positive = 40\n",
      "False Negative = 87\tTrue Positive = 226\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.86      0.80       287\n",
      "           1       0.85      0.72      0.78       313\n",
      "\n",
      "    accuracy                           0.79       600\n",
      "   macro avg       0.79      0.79      0.79       600\n",
      "weighted avg       0.80      0.79      0.79       600\n",
      "\n",
      "Training and Evaluating Random Forests Classifier on TF-IDF Features\n",
      "Best parameters are: {'C': 100, 'solver': 'lbfgs'}\n",
      "Best F1 score is: 0.9202036918852146\n",
      "Accuracy of Test Set is 0.7966666666666666\n",
      "\n",
      "\n",
      "True Negative = 249\tFalse Positive = 38\n",
      "False Negative = 84\tTrue Positive = 229\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.87      0.80       287\n",
      "           1       0.86      0.73      0.79       313\n",
      "\n",
      "    accuracy                           0.80       600\n",
      "   macro avg       0.80      0.80      0.80       600\n",
      "weighted avg       0.81      0.80      0.80       600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print('BOW model:> Train Features Shape:', X_train_CV.shape, ' Test Features Shape:', X_test_CV.shape)\n",
    "print('TF-IDF model:> Train Features Shape:', X_train_Tf.shape, ' Test Features Shape:', X_test_Tf.shape)\n",
    "\n",
    "\n",
    "print(\"Training and Evaluating Random Forests Classifier on CV Features\")\n",
    "\n",
    "# Grid Search with Cross Validation for CV Features\n",
    "params = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'solver': ['newton-cg', 'lbfgs'],\n",
    "}\n",
    "\n",
    "clf_CV = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "model_CV = GridSearchCV(clf_CV, params, cv=5, scoring='average_precision')\n",
    "model_CV.fit(X_train_CV, y_train)\n",
    "\n",
    "print('Best parameters are:', model_CV.best_params_)\n",
    "print('Best F1 score is:', model_CV.best_score_)\n",
    "\n",
    "clf_CV = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    C=model_CV.best_params_['C'],\n",
    "    solver=model_CV.best_params_['solver'],\n",
    "    random_state=42,\n",
    "    max_iter=1000)\n",
    "\n",
    "clf_CV.fit(X_train_CV, y_train)\n",
    "\n",
    "model_evaluation(clf_CV, X_test_CV, y_test)\n",
    "\n",
    "\n",
    "print(\"Training and Evaluating Random Forests Classifier on TF-IDF Features\")\n",
    "\n",
    "# Grid search with cross validation for TF-IDF features\n",
    "clf_Tf = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "model_Tf = GridSearchCV(clf_Tf, params, cv=5, scoring='average_precision')\n",
    "model_Tf.fit(X_train_Tf, y_train)\n",
    "\n",
    "print('Best parameters are:', model_Tf.best_params_)\n",
    "print('Best F1 score is:', model_Tf.best_score_)\n",
    "\n",
    "clf_Tf = LogisticRegression(\n",
    "    class_weight='balanced',\n",
    "    C=model_Tf.best_params_['C'],\n",
    "    solver=model_Tf.best_params_['solver'],\n",
    "    random_state=42,\n",
    "    max_iter=1000)\n",
    "\n",
    "clf_Tf.fit(X_train_Tf, y_train)\n",
    "model_evaluation(clf_Tf, X_test_Tf, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. Given the accuracy and F1-score of your model, are you satisfied with the results, from a business point of view? Explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here.\n",
    "\n",
    "The accuracy of the test dataset using Random Forests Classifier on CL is 78%, with the F1 Score of 91%.  \n",
    "The accuracy of the test dataset using Random Forests Classifier on TD-IDF is 80%, with the F1 Score of 92%.\n",
    "\n",
    "This shows that without using a much larger corpus to train on, ML algorithms won't do well on sentiment analysis, which involves being able to analyze the structure of words in each sentence and how they relate to one another. The provided training set of 2,400 sentences is too small to train a model properly and get decent results on the test set of 600 sentences. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Show five example instances in which your model’s predictions were incorrect. Describe why you think the model was wrong. Don’t just guess: dig deep to figure out the root cause."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Insert your answer here.\n",
    "\n",
    "I printed 25 incorrect predictions and selected the five below to discuss:\n",
    "\n",
    "Examples of false negatives:\n",
    "1)\tThe soundtrack wasn't terrible, either.\n",
    "Why the model was labled this as 1 which is wrong: It seems that the word \"terrible\" caused the model to classify this sentence incorrect as negative, without being able to understand that “wasn’t” reverses the overall sentiment of the sentence. The word2vec processing step of our model was unable to capture word order.\n",
    "\n",
    "2)\tI won't say any more - I don't like spoilers, so I don't want to be one, but I believe this film is worth your time.\n",
    "Why the model was wrong: It looks as if “won't” and repeated “don't” are causing the model to classify this sentence polarity incorrectly as negative. This is a difficult sentence to analyze, as there are repeated negtive words which impacts the sentiment while one word \"worth\" gives a postive sentiment to the entire sentence.\n",
    "\n",
    "3)\tMy 8/10 score is mostly for the plot.\n",
    "Why the model was wrong: The model was not trained to handle and ratios for specified rating system, in which a score 8 out of 10 would be considered positive. Handling rating system is not a trivial task, as only humans know that 10 is good and 1 is bad and the rate between 1 to 10 have different sentiment.\n",
    "\n",
    "Examples of false positives:\n",
    "4)\tThe directing is sloppy at best.\n",
    "Why the model was wrong: Probably the word best was the determiment to give the senteince positive sentiment. The negative sentiment here can be interpreted by combination of words (“sloppy at best”) which this model is not equipped to do, as the word2vec processing step does not capture word order.\n",
    "\n",
    "5)\tI'm terribly disappointed that this film would receive so many awards and accolades, especially when there are far more deserving works of film out there.   \n",
    "Why the model was wrong: The model seems to be placing emphasis on \"awards\" and \"accolades\", without being able to handle \"there are far more deserving works\". This is a difficult sentence to get correctly as the sentiment is hiddent in the long context and the model is not sophisticated enough to handle its intricacies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 25 Misclassified Sentences\n",
      "                                             sentence  label  actual_label\n",
      "0   Not too screamy not to masculine but just righ...      0             1\n",
      "1   I would have casted her in that role after rea...      0             1\n",
      "2           The soundtrack wasn't terrible, either.        0             1\n",
      "3   Still, it was the SETS that got a big \"10\" on ...      0             1\n",
      "4   The last 15 minutes of movie are also not bad ...      0             1\n",
      "5             My 8/10 score is mostly for the plot.        0             1\n",
      "6   I won't say any more - I don't like spoilers, ...      0             1\n",
      "7   If only someone involved with it knew how to s...      1             0\n",
      "8   Whatever the producer was going for, he missed...      1             0\n",
      "9                  The directing is sloppy at best.        1             0\n",
      "10  The acting by the whole cast could be put on a...      1             0\n",
      "11  And, FINALLY, after all that, we get to an end...      1             0\n",
      "12  Lifetime does not air it enough, so if anyone ...      0             1\n",
      "13                 But this movie really got to me.        0             1\n",
      "14                                          See it.        0             1\n",
      "15  I really hope the team behind this movie makes...      0             1\n",
      "16  Later I found myself lost in the power of the ...      0             1\n",
      "17  I left the theater with a lilt in my step, joy...      0             1\n",
      "18  Full of unconvincing cardboard characters it i...      1             0\n",
      "19  Whatever prompted such a documentary is beyond...      1             0\n",
      "20  This second appearance of Mickey Mouse (follow...      0             1\n",
      "21  While you don't yet hear Mickey speak, there a...      0             1\n",
      "22  I'm terribly disappointed that this film would...      1             0\n",
      "23  It's a shame to see good actors like Thomerson...      1             0\n",
      "24  The film's sole bright spot was Jonah Hill (wh...      0             1\n",
      "...End of Question 2-Sentiment Analysis via the ML-based approach\n"
     ]
    }
   ],
   "source": [
    "# TODO: Feel free to use code as well to answer this question. Or not. Up to you.\n",
    "\n",
    "# Identify 25 wrong predictions....\n",
    "pd.option_context('display.max_colwidth', -1)\n",
    "print(\"Writing 25 Misclassified Sentences\")\n",
    "\n",
    "misclassified_sentences = []\n",
    "count = 0\n",
    "for ind, sentence in enumerate(test_data['Sentence']):\n",
    "    norm_sentence = normalize_doc(sentence)\n",
    "    sentence_Tf = tv.transform([norm_sentence])\n",
    "    y_pred = model_Tf.predict(sentence_Tf)[0]\n",
    "    if y_pred != y_test[ind]:\n",
    "        misclassified_sentences.append(\n",
    "            {\n",
    "                'sentence': sentence,\n",
    "                'label': y_pred,\n",
    "                'actual_label': y_test[ind]\n",
    "            }\n",
    "        )\n",
    "\n",
    "misclassified_sentences = pd.json_normalize(\n",
    "                    misclassified_sentences[0:25]\n",
    "                    )\n",
    "print(misclassified_sentences[0:25])\n",
    "misclassified_sentences.columns = ['Sentence', 'Predicted Label', 'Actual Label']\n",
    "misclassified_sentences.to_csv('MMA865-BigData-Sentiment-Q2.csv')\n",
    "\n",
    "print('...End of Question 2-Sentiment Analysis via the ML-based approach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mma",
   "language": "python",
   "name": "mma"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
